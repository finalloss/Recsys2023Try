# Recsys Challenge 2023 推荐系统挑战赛项目

本项目是为 [Recsys Challenge 2023](https://sharechat.com/recsys2023) 提交的解决方案。该竞赛旨在探索推荐算法在Sharechat 和 Moj 应用提供的真实大规模广告数据集上的性能，为推荐系统领域的研究和应用提供创新性的解决方案，尤其关注用户隐私和深度漏斗优化。

## 团队信息

* **队伍名称:** Baker street Irregulars

## 任务描述

本次挑战赛的任务是预测在给定的广告展示下，用户是否会点击广告 (`is_clicked`) 以及是否会安装应用 (`is_installed`)。训练数据由过去两周的二次抽样印象、点击和安装组成，目标是预测第15天的安装概率。

数据集包含约1000万随机用户的行为记录，特征是匿名的，包括类别特征和数值特征，不提供具体语义。

**评价指标**：比赛使用归一化交叉熵 (Normalised-Entropy) 来衡量模型预测的准确性。

## 解决方案

我们的解决方案将此问题视为一个分类任务，并采用模型集成的策略来提高预测的准确性和鲁棒性。

### 1. 数据预处理与特征工程

由于训练集文件较多，我们使用 `glob` 进行统一读取。核心步骤如下：
* **特征选择**: 我们首先计算了所有特征与目标标签 (`is_installed`) 之间的相关性，并筛选出相关性绝对值大于0.02的特征用于模型训练。这帮助我们在不理解特征具体语义的情况下，识别出对预测结果影响较大的特征。
* **数据处理**:
    * 将训练集和测试集合并，对类别特征进行统一的标签编码 (`LabelEncoder`)。
    * 对数值特征进行标准化处理。
    * 使用 `train_test_split` 将处理后的数据划分为训练集和验证集。

### 2. 模型架构

我们采用了两种强大的模型进行集成：

* **DeepFM (深度因子分解机)**: 该模型能够同时学习低阶和高阶的特征交互，无需复杂的特征工程。我们将原始特征输入共享的Embedding层，然后分别送入FM部分和DNN部分。
    * **FM部分**: 捕捉一阶和二阶的特征交叉。
    * **DNN部分**: 通过多层前馈神经网络学习高阶的复杂特征关系。
    * 最终将两部分的输出结合，通过Sigmoid函数得到预测概率。

* **XGBoost (eXtreme Gradient Boosting)**: 这是一个高效且强大的集成学习算法。我们使用 `XGBClassifier` 对筛选后的特征进行训练。XGBoost通过构建多个决策树并将其结果进行组合，能够有效地处理稀疏数据并获得很高的预测精度。

### 3. 模型集成

为了得到最终的预测结果，我们将DeepFM和XGBoost两个模型输出的预测概率进行加权平均。经过多次提交测试，我们发现简单的算术平均能取得最好的效果。

## 项目亮点与创新

* **模型融合**: 创造性地将深度学习模型 (DeepFM) 与传统的集成学习模型 (XGBoost) 相结合。DeepFM擅长自动学习复杂的特征交互，而XGBoost在处理表格数据方面表现出色。两者的结合使得模型更加稳健，优势互补。
* **自动化特征筛选**: 在特征完全匿名的挑战下，我们没有进行盲目的人工特征组合，而是通过计算特征与标签的相关性，自动化地筛选出重要特征，为模型训练提供了更有价值的输入。
* **端到端的解决方案**: 项目包含了从数据分析、预处理、特征选择、模型训练、调参到模型集成的完整流程。

## 最终成绩

我们的解决方案在 **Academia Leaderboard** 上取得了优异的成绩：

* **最终得分**: 6.342383
* **最终排名**: **8 / 54**
